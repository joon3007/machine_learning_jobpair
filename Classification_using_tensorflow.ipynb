{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Job Pair (Classification)\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requirement\n",
    "* python 2.x.x                                                         \n",
    "\n",
    "* Tensorflow 2.0  \n",
    "\n",
    "* CUDA 10.2  \n",
    "\n",
    "* Cudnn >= 7.6.2  \n",
    "\n",
    "* Pillow, SciPy, six, numpy, OpenCV 3.x.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment  \n",
    "\n",
    "* Ubuntu 18.04\n",
    "* Nvidia-docker 2.0.3\n",
    "* GeForce GTX1070 8G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation  \n",
    "* Traing 할수있는 데이터의 수를 늘리고자 회전, 가우시안 노이즈를 적용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os\n",
    "import cv2\n",
    "import preprocess.augmentation as aug #가우시안 노이즈를 사용하기위한 외부소스\n",
    "\n",
    "data_file_path = \"/data/faces_images\" # 데이터의 위치와 훈련데이터 정보가 있는 csv파일 경로\n",
    "label_file_path = \"/data/train_vision.csv\" \n",
    "\n",
    "f = open('/data/noise_train_vision.csv','w')\n",
    "wr = csv.writer(f)\n",
    "wr.writerow(['filename','label'])\n",
    "\n",
    "with open(label_file_path) as path_list:\n",
    "    for data in enumerate(path_list):\n",
    "        if data[0] is 0: continue\n",
    "        filename, label = data[1].split(\",\")\n",
    "        filepath = os.path.join(data_file_path,filename)\n",
    "        \n",
    "        ### 가우시안 노이즈 이미지 생성\n",
    "        img = cv2.imread(filepath)\n",
    "        noised_img = aug.gaussian_noise(img, mean=0, var =10) \n",
    "        \n",
    "        ### Label파일 생성\n",
    "        wr.writerow([filename, int(label)])\n",
    "        cv2.imwrite(os.path.join(data_file_path,'noised_'+filename), noised_img)\n",
    "        wr.writerow(['noised_'+filename, int(label)])\n",
    "        \n",
    "        '''\n",
    "        for i in range(1,4):\n",
    "            img = img.rotate(90)\n",
    "            img.save(os.path.join(data_file_path,'rot'+str(90*i)+'_'+filename))\n",
    "            wr.writerow(['rot'+str(90*i)+'_'+filename, int(label)])\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "* tensorflow keras의 데이터생성 함수의 경우 폴더 내부의 이미지를 읽어들여 Data Generator를 생성하기 때문에 이미지의 파일경로와 라벨을 가지고 데이터를 생성하는 flow_from_mapfile 구현.  \n",
    "<br>\n",
    "* Feature 잘 분류하기 위해 데이터 처리 과정에서 horizontal flip, zoom, shear, rescale 적용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "Found 11700 images for training.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import preprocess.image as image\n",
    "import tensorflow.keras.utils as utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import GeneratorEnqueuer\n",
    "from tensorflow.keras.utils import OrderedEnqueuer\n",
    "print (tf.__version__)\n",
    "import random\n",
    "\n",
    "\n",
    "data_file_path = \"/data/faces_images\"\n",
    "label_file_path = \"/data/noise_train_vision.csv\"\n",
    "initial_learning_rate = 0.001 \n",
    "image_size = 299\n",
    "class_num = 6\n",
    "batch_size = 16\n",
    "epoch = 30\n",
    "\n",
    "### GPU Memory 사용량을 제어하기 위한 코드 \n",
    "### 기존의 tensorflow 1.x 버전의 Session을 이용하여 할당하는데 필요한 memory만을 사용.\n",
    "'''\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.compat.v1.Session(config=config))\n",
    "'''\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "\n",
    "### 데이터 생성을 위한 path와 label리스트 생성\n",
    "filenames = []\n",
    "labels = []\n",
    "with open(label_file_path) as path_list:\n",
    "    for data in enumerate(path_list):\n",
    "        if data[0] is 0:\n",
    "            continue\n",
    "            \n",
    "        filename, label = data[1].split(\",\")\n",
    "        filenames.append(os.path.join(data_file_path,filename))\n",
    "        labels.append(int(label)-1)\n",
    "        \n",
    "    '''\n",
    "    random.seed(1)    \n",
    "    random.shuffle(filenames)\n",
    "    random.shuffle(labels)\n",
    "    '''\n",
    "    \n",
    "    print('Found %d images for training.' % (len(filenames)))\n",
    "    \n",
    "class_num = list(range(class_num))\n",
    "\n",
    "'''\n",
    "Class ImageDataGenerator\n",
    "- Training에 사용되는 데이터를 생성하고 queue에 전달해주는 iterator.\n",
    "  전달시 설정한 범위에서 random으로 filp, zoom, shearing을 수행하여 데이터를 보낸다.\n",
    "'''\n",
    "train_datagen = image.ImageDataGenerator(rescale=1. / 255,\n",
    "                                        shear_range=0.2,\n",
    "                                        zoom_range=0.2,\n",
    "                                        horizontal_flip=True)\n",
    "\n",
    "'''\n",
    "Class DataPathIterator (기존의 DirectoryInterator를 수정)\n",
    "- flow_from_mapfile의 return값. filename리스트를 읽어 데이터로 변환하며 random index를 통해 Training시 data batch\n",
    "  를 random하게 구성하여 전달한다.\n",
    "\n",
    "argment : \n",
    "        get_batches_of_transformed_samples : random index생성 및 이미지 로드를 하며 각 데이터 라벨에 대해 \n",
    "        one-hot encording을 수행한다.\n",
    "        \n",
    "        next : 데이터의 다음 batch를 반환.\n",
    "'''\n",
    "train_generator = train_datagen.flow_from_mapfile(class_num,filenames,labels,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=batch_size,\n",
    "                                            class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch : 4, loss : 1.024148, accuracy : 0.750000, step : 3112/21937 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:62: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch : 24, loss : 0.052964, accuracy : 1.000000, step : 20427/21937 "
     ]
    }
   ],
   "source": [
    "import net.nets_factory as nets_factory\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "\n",
    "#os.system(\"export TF_FORCE_GPU_ALLOW_GROWTH = true\")\n",
    "network_name = 'inception_v3'\n",
    "save_model_path = \"/home/june/model\"\n",
    "network_fn = nets_factory.get_network(network_name)\n",
    "model = network_fn(include_top=True, weights=None, classes=len(class_num))\n",
    "\n",
    "\n",
    "lr_schedule = optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                                                    decay_steps=40000,\n",
    "                                                    decay_rate=0.96,\n",
    "                                                    staircase=True)\n",
    "\n",
    "#optimizer = optimizers.SGD(learning_rate = lr_schedule)\n",
    "#optimizer = optimizers.RMSprop(learning_rate = lr_schedule)\n",
    "optimizer = optimizers.Adam(learning_rate = lr_schedule, beta_1 = 0.9,\n",
    "                            beta_2 = 0.999, epsilon = 1e-08, decay= 0.)\n",
    "#optimizer = optimizers.Adagrad(learning_rate = lr_schedule)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "#model.load_weights(os.path.join(save_model_path,\"inception_v3_10.ckpt\"))\n",
    "\n",
    "def create_data_queue(train_generator):\n",
    "    is_sequence = isinstance(train_generator, Sequence)\n",
    "    wait_time = 0.01\n",
    "    if is_sequence:\n",
    "        enqueuer = OrderedEnqueuer(train_generator,\n",
    "                                use_multiprocessing=False,\n",
    "                                shuffle = True)\n",
    "    else:\n",
    "        enqueuer = GeneratorEnqueuer(train_generator,\n",
    "                                use_multiprocessing =False,\n",
    "                                wait_time = wait_time)\n",
    "    return enqueuer\n",
    "\n",
    "\n",
    "\n",
    "enqueuer = create_data_queue(train_generator)\n",
    "enqueuer.start(workers = 1, max_queue_size=10)\n",
    "output_generator = enqueuer.get()\n",
    "accuracy_mean = 0.\n",
    "current_step = 0\n",
    "current_epoch = 1\n",
    "max_iter = len(filenames) / batch_size\n",
    "\n",
    "try:\n",
    "    while current_step < epoch * max_iter:\n",
    "        if current_step%max_iter == 0:\n",
    "            accuracy_mean = 0.\n",
    "            current_epoch = int(current_step/max_iter)\n",
    "            \n",
    "            \n",
    "        generator_output = next(output_generator)\n",
    "        x, y = generator_output\n",
    "        loss, accuracy = model.train_on_batch(x,y)            \n",
    "        accuracy_mean = ((accuracy_mean * current_step-(max_iter*current_epoch))+accuracy) / (current_step-(max_iter*current_epoch)+1)\n",
    "                \n",
    "        sys.stdout.write('\\r>> epoch : %d, loss : %f, accuracy : %f, step : %d/%d ' % (current_epoch, loss, accuracy, current_step, epoch * max_iter))\n",
    "        sys.stdout.flush()\n",
    "        current_step += 1\n",
    "        \n",
    "        if current_step%1000 ==0 :\n",
    "            model.save_weights(os.path.join(save_model_path,'%s_%s_%d.ckpt'%('Adam',network_name,epoch)))\n",
    "            \n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    enqueuer.stop(timeout=0.01)\n",
    "    exit()\n",
    "    \n",
    "enqueuer.stop(timeout=0.01)\n",
    "model.save_weights(os.path.join(save_model_path,'%s_%s_%d.ckpt'%('Adam',network_name,epoch)))\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n",
      "[1, 4]\n",
      "[1, 4]\n",
      "[1, 4, 1]\n",
      "[1, 4, 1]\n",
      "[1, 4, 1, 1]\n",
      "[1, 4, 1, 1]\n",
      "[1, 4, 1, 1, 4]\n",
      "[1, 4, 1, 1, 4]\n",
      "[1, 4, 1, 1, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4, 1, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4, 1, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4, 1, 1, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4, 1, 1, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4, 1, 1, 4, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4, 1, 1, 4, 1]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4, 1, 1, 4, 1, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4, 1, 1, 4, 1, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4, 1, 1, 4, 1, 4, 4]\n",
      "[1, 4, 1, 1, 4, 4, 4, 5, 4, 1, 4, 5, 4, 2, 4, 4, 4, 1, 2, 1, 1, 4, 4, 4, 4, 5, 4, 4, 1, 1, 4, 1, 4, 4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-868b442d0282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0msoftmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0msoftmax2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     return self._model_iteration(\n\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.pyc\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    445\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.pyc\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mbatch_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             batch_end=step * batch_size + current_batch_size)\n\u001b[0;32m--> 173\u001b[0;31m       \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m       \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/callbacks.pyc\u001b[0m in \u001b[0;36mmake_logs\u001b[0;34m(model, logs, outputs, mode, prefix)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   \u001b[0;34m\"\"\"Computes logs for sending to `on_batch_end` methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0mmetric_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmetric_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36mmetrics_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;31m# Add all metric names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mmetrics_names\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    412\u001b[0m       \u001b[0mmetrics\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_metric_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_metrics_from_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_get_metrics_from_layers\u001b[0;34m(layers)\u001b[0m\n\u001b[1;32m   3241\u001b[0m       \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_metrics_from_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3242\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3243\u001b[0;31m       \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3244\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_subclass_implementers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36m_gather_children_attribute\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m   2331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_layers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m       nested_layers = trackable_layer_utils.filter_empty_layer_containers(\n\u001b[0;32m-> 2333\u001b[0;31m           self._layers)\n\u001b[0m\u001b[1;32m   2334\u001b[0m       return list(\n\u001b[1;32m   2335\u001b[0m           itertools.chain.from_iterable(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/tracking/layer_utils.pyc\u001b[0m in \u001b[0;36mfilter_empty_layer_containers\u001b[0;34m(layer_list)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mfiltered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m       \u001b[0;31m# Trackable data structures will not show up in \".layers\" lists, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0;31m# the layers they contain will.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/tracking/data_structures.pyc\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0;31m# in particular seems to look up properties on the wrapped object instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m       \u001b[0;31m# of the wrapper without this logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_DictWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/tracking/data_structures.pyc\u001b[0m in \u001b[0;36mlayers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_empty_layer_containers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/tracking/data_structures.pyc\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0;31m# in particular seems to look up properties on the wrapped object instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m       \u001b[0;31m# of the wrapper without this logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_DictWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/tracking/data_structures.pyc\u001b[0m in \u001b[0;36m_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# they're wrapping if out of sync.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mcollected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m       if (isinstance(obj, TrackableDataStructure)\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mor\u001b[0m \u001b[0mlayer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/tracking/data_structures.pyc\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0;31m# in particular seems to look up properties on the wrapped object instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m       \u001b[0;31m# of the wrapper without this logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_DictWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/tracking/data_structures.pyc\u001b[0m in \u001b[0;36m_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;34m\"\"\"Collect values for TrackableDataStructure.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;31m# Sort items deterministically by key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m     \u001b[0mordered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_file_path = \"/data/test_vision.csv\"\n",
    "import net.nets_factory as nets_factory\n",
    "from tensorflow.keras.preprocessing import image as kimage\n",
    "import csv, os\n",
    "import numpy as np\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import collections\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "def load_image(image_file_path):\n",
    "        img = kimage.load_img(image_file_path, target_size=(image_size, image_size))\n",
    "        x = kimage.img_to_array(img)\n",
    "        x /= 255.\n",
    "        return x\n",
    "    \n",
    "result = []\n",
    "result2 = []\n",
    "data_file_path = \"/data/faces_images\"\n",
    "save_model_path = \"/home/june/model\"\n",
    "network_fn = nets_factory.get_network('inception_resnet_v2')\n",
    "network_fn2 = nets_factory.get_network('inception_resnet_v2')\n",
    "model = network_fn(include_top=True, weights=None, classes=6)\n",
    "model2 = network_fn2(include_top=True, weights=None, classes=6)\n",
    "image_size = 299\n",
    "\n",
    "lr_schedule = optimizers.schedules.ExponentialDecay(0.001,\n",
    "                                                    decay_steps=15000,\n",
    "                                                    decay_rate=0.96,\n",
    "                                                    staircase=True)\n",
    "\n",
    "#optimizer = optimizers.SGD(learning_rate = lr_schedule)\n",
    "#optimizer = optimizers.Adagrad(learning_rate = lr_schedule)\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate = lr_schedule, beta_1 = 0.9,\n",
    "                            beta_2 = 0.999, epsilon = 1e-08, decay= 0.)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.load_weights(os.path.join(save_model_path,\"Adam_inception_resnet_v2_20.ckpt\"))\n",
    "model2.load_weights(os.path.join(save_model_path,\"Adam_inception_resnet_v2_20.ckpt\"))\n",
    "count = collections.Counter()\n",
    "stack = []\n",
    "with open(test_file_path) as path_list:\n",
    "    for image_path in enumerate(path_list):\n",
    "        if image_path[0] is 0 : continue\n",
    "        img = load_image(os.path.join(data_file_path,image_path[1].split(\"\\n\")[0]))\n",
    "        x = np.expand_dims(img, axis=0)\n",
    "        softmax = model.predict(x, batch_size=1)\n",
    "        softmax2 = model.predict(x, batch_size=1)\n",
    "        result.append(np.asarray(softmax).argmax() + 1)\n",
    "        result2.append(np.asarray(softmax).argmax() + 1)\n",
    "        print(result)\n",
    "        print(result2)\n",
    "        \n",
    "        exit()\n",
    "#df = pd.DataFrame(result, columns=['label'])\n",
    "#df.to_csv(\"/data/result.csv\")\n",
    "f = open('/data/result_adam_inception_res.csv','w')\n",
    "wr = csv.writer(f)\n",
    "wr.writerow(['prediction'])\n",
    "for i in result:\n",
    "    wr.writerow([i])\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
